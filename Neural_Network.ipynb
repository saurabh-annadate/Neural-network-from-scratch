{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Chq5cVUnkvw8"
   },
   "source": [
    "**Assignment:2**\n",
    "\n",
    "# **Work by:** Saurabh Annadate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSt6h7PIkvxD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7m_ONrW-kvxP"
   },
   "source": [
    "1) Loading data.\n",
    "\n",
    "2) Making a function which will return the pixel values and corresponding digit in X,Y format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBaVGIyykvxS"
   },
   "outputs": [],
   "source": [
    "def load_data(path='https://raw.githubusercontent.com/seaborncat/ML-Project/master/mnist_train.csv'): #github link to the training data provided\n",
    "    f = pd.read_csv(path)\n",
    "    tr_x=np.array(f.iloc[:,:-1])\n",
    "    tr_y=np.array(f.iloc[:,-1])\n",
    "    X = [np.reshape(x, (784, 1))/255 for x in tr_x]      # X matrix of training dataset\n",
    "    Y = [vectorized_result(y) for y in tr_y]            # Y matrix of training dataset (hot-encoded instead of real numbers)\n",
    "    return (X,Y)                                        #it will be easier to handle hot-encoded,we can output real numbers also if we want\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qVvqt97jkvxb"
   },
   "outputs": [],
   "source": [
    "X,Y=load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2AFLDUMkvxj"
   },
   "source": [
    "**Building the neural network of zero hidden layers...**\n",
    "\n",
    "784>10 \n",
    "\n",
    "1)Minimization function used here is cross-entropy.\n",
    "\n",
    "2)Output neurons are based on softmax function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvbQyBvvkvxm"
   },
   "outputs": [],
   "source": [
    "def update_weights_perceptrons( X,Y,weights,biases, eta): \n",
    "        mini_batch_size=10000 #mini_batch_size for stochastic GD (here taken complete data for this example in a single step)\n",
    "        training_data = list(zip(X,Y))\n",
    "        n = len(training_data)\n",
    "        random.shuffle(training_data)\n",
    "        mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "        for mini_batch in mini_batches:           #updates weights and biases here..\n",
    "            grad_b = [np.zeros(b.shape) for b in biases]\n",
    "            grad_w = [np.zeros(w.shape) for w in weights]\n",
    "            for x, y in mini_batch:\n",
    "                delta_grad_b, delta_grad_w = backprop(x, y,weights,biases)\n",
    "                grad_b = [nb+dnb for nb, dnb in zip(grad_b, delta_grad_b)]\n",
    "                grad_w = [nw+dnw for nw, dnw in zip(grad_w, delta_grad_w)]\n",
    "            weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(weights, grad_w)]\n",
    "            biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(biases, grad_b)]\n",
    "            \n",
    "        return weights,biases\n",
    "\n",
    "def backprop( x, y,weights,biases):\n",
    "    grad_b = [np.zeros(b.shape) for b in biases]  #grad_b and grad_w are small steps towards minima of b and w\n",
    "    grad_w = [np.zeros(w.shape) for w in weights]\n",
    "    # feedforward\n",
    "    activation = x\n",
    "    activations = [x]            # list to store all the activations, layer by layer\n",
    "    zs = []                      # list to store all the z vectors, layer by layer\n",
    "    for b, w in zip(biases, weights):\n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = softmax(z)\n",
    "        activations.append(activation)\n",
    "        #backprop starts \n",
    "        delta = delta1(zs[-1], activations[-1], y)  #first argument tells the layer number(last layer in this case)\n",
    "        grad_b[-1] = delta\n",
    "        grad_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "    return (grad_b, grad_w)\n",
    "\n",
    "    \n",
    "\n",
    "#extra functions used\n",
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)\n",
    "def fn(a, y):\n",
    "    return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a))) #crossentropy \n",
    "def delta1(z, a, y): #this is an error between actual value and last activation\n",
    "    return (a-y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhr9iLytkvxu"
   },
   "outputs": [],
   "source": [
    "sizes=np.array([784,10])   #initializing weights and biases for given layers\n",
    "biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zf_1rRsakvx1"
   },
   "outputs": [],
   "source": [
    "w,b=update_weights_perceptrons(X,Y,weights,biases,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "colab_type": "code",
    "id": "qYX7z_Eskvx7",
    "outputId": "611ad66c-4cf2-46b4-a27a-9ff11cf1ff32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.76854653,  1.53138716, -0.38072967, ...,  0.3148166 ,\n",
      "         0.45139251,  0.1935604 ],\n",
      "       [-0.65682131,  0.51557157,  1.50927265, ..., -2.01046924,\n",
      "        -0.44023945,  0.04889727],\n",
      "       [-0.41455056, -0.91405244,  1.40586892, ...,  0.33076941,\n",
      "        -0.08462456,  1.58111736],\n",
      "       ...,\n",
      "       [-0.78385053,  0.47677477,  1.1413737 , ...,  0.07966108,\n",
      "        -0.84374008,  0.53009183],\n",
      "       [ 0.31945697,  0.93940009, -1.87631725, ..., -2.39040426,\n",
      "        -0.59044523, -1.61732873],\n",
      "       [-0.89357978, -0.18336752, -0.35836683, ..., -0.39673404,\n",
      "         0.27951699, -0.67444063]])]\n",
      "[array([[-0.83758476],\n",
      "       [ 0.39065983],\n",
      "       [-0.52946842],\n",
      "       [-0.87705436],\n",
      "       [-0.3645372 ],\n",
      "       [-0.33686772],\n",
      "       [ 1.75075929],\n",
      "       [ 1.25311792],\n",
      "       [ 0.82435307],\n",
      "       [-0.14928285]])]\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGrL7x5FkvyI"
   },
   "outputs": [],
   "source": [
    "#let's define all the functions we'll use in future here..\n",
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)\n",
    "def fn(a, y):\n",
    "    return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "def delta1(z, a, y):\n",
    "    return (a-y)\n",
    "def softmax_grad(s): \n",
    "    jacobian_m = np.diag(s)   \n",
    "    for i in range(len(jacobian_m)):\n",
    "        for j in range(len(jacobian_m)):\n",
    "            if i == j:\n",
    "                jacobian_m[i][j] = s[i] * (1-s[i])\n",
    "            else: \n",
    "                jacobian_m[i][j] = -s[i]*s[j]\n",
    "    return jacobian_m\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "def tanh(z):\n",
    "    return np.tanh(z) \n",
    "def tanh_grad(z):\n",
    "    return 1-np.square(np.tanh(z))\n",
    "def relu(z, deriv = False):   #fixed relu (will not blow ar z=0/1 because changed to z=0.0001/0.999)\n",
    "    if not deriv:\n",
    "        relud = z/255\n",
    "        relud[relud < 0] = 0\n",
    "        return relud\n",
    "    deriv = z\n",
    "    deriv[deriv <= 0] = 0\n",
    "    deriv[deriv > 0] = 1\n",
    "    return deriv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5OzZtmFFkvyP"
   },
   "source": [
    "Neural Network with 1 hidden layer...\n",
    "\n",
    "**784>10>10**\n",
    "\n",
    "hidden layer uses sigmoid as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5QkuZCEkvyR"
   },
   "outputs": [],
   "source": [
    "def update_weights_single_layer( X,Y,weights,biases, eta): \n",
    "        ''' Using mini batches in gradient descent from now\n",
    "        because its computationally efficient and takes lesser time'''\n",
    "        mini_batch_size=10 \n",
    "        training_data = list(zip(X,Y))\n",
    "        n = len(training_data)\n",
    "        random.shuffle(training_data)\n",
    "        mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "        for mini_batch in mini_batches:\n",
    "            grad_b = [np.zeros(b.shape) for b in biases]\n",
    "            grad_w = [np.zeros(w.shape) for w in weights]\n",
    "            for x, y in mini_batch:\n",
    "                delta_grad_b, delta_grad_w = backprop(x, y,weights,biases)\n",
    "                grad_b = [nb+dnb for nb, dnb in zip(grad_b, delta_grad_b)]\n",
    "                grad_w = [nw+dnw for nw, dnw in zip(grad_w, delta_grad_w)]\n",
    "            weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(weights, grad_w)]\n",
    "            biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(biases, grad_b)]\n",
    "            \n",
    "        return weights,biases\n",
    "\n",
    "def backprop( x, y,weights,biases):\n",
    "    grad_b = [np.zeros(b.shape) for b in biases]  #grad_b and grad_w are small steps towards minima of b and w\n",
    "    grad_w = [np.zeros(w.shape) for w in weights]\n",
    "    # feedforward\n",
    "    activation = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    \n",
    "    for b, w in zip(biases[:-1], weights[:-1]):  #activations upto second last layer using sigmoid\n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)     \n",
    "    for b, w in zip(biases[-1:], weights[-1:]): #activation of the last layer using softmax  \n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = softmax(z)\n",
    "        activations.append(activation)\n",
    "    delta = delta1(zs[-1], activations[-1], y)     #backprop from softmax layer\n",
    "    grad_b[-1] = delta\n",
    "    grad_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    for l in range(2, len(biases)+1):              #backprop from  rest of the layer\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_prime(z)\n",
    "        delta = np.dot(weights[-l+1].transpose(), delta) *sp  \n",
    "        grad_b[-l] = delta\n",
    "        grad_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    return (grad_b, grad_w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hrkipG3kvyY"
   },
   "outputs": [],
   "source": [
    "sizes1=np.array([784,10,10])\n",
    "biases1 = [np.random.randn(y, 1) for y in sizes1[1:]]\n",
    "weights1 = [np.random.randn(y, x) for x, y in zip(sizes1[:-1], sizes1[1:])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 903
    },
    "colab_type": "code",
    "id": "6PNMxzO0kvyf",
    "outputId": "e23006b2-3451-4651-9c4c-c61cb6298a6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.19558433,  1.5947903 , -0.69658389, ..., -2.43648752,\n",
      "         1.07612152, -1.48686689],\n",
      "       [-0.54567876,  0.67134208,  0.29820422, ...,  0.86704487,\n",
      "        -1.60651281, -0.70651083],\n",
      "       [-1.5870352 , -0.68850767, -0.08476187, ..., -1.43377292,\n",
      "         0.58210878, -1.41604227],\n",
      "       ...,\n",
      "       [ 0.86605891,  0.81242171,  1.8300582 , ...,  0.44594629,\n",
      "        -0.81481453, -1.7043912 ],\n",
      "       [-0.15025627,  1.54554692,  0.08987386, ..., -0.61524367,\n",
      "         0.00887965, -0.96974795],\n",
      "       [ 0.09670126, -0.81654878, -0.23200942, ..., -1.0191568 ,\n",
      "        -0.68041648, -0.37926725]]), array([[ 0.52807677, -2.93518663, -1.97061199,  1.758346  , -1.36450569,\n",
      "        -1.81060278,  4.08901211, -1.04289101,  2.23053977, -1.6761527 ],\n",
      "       [-1.38379237,  1.62691553, -1.74055665,  2.21597915,  1.4542596 ,\n",
      "         2.19449691, -1.11558785, -2.57081136, -2.36026424,  2.32183524],\n",
      "       [-0.9307489 , -1.4468928 ,  0.02871845, -0.90373804,  0.93616759,\n",
      "        -0.63146346,  2.19626668, -1.47807814, -0.81625352,  3.49062727],\n",
      "       [ 2.4179652 , -0.48560045, -0.74611909,  1.83142111,  0.41138636,\n",
      "        -2.21373909, -1.9361262 , -1.10556459,  1.0651171 ,  3.15006582],\n",
      "       [ 0.30152852, -1.51566618,  0.98530197, -2.62498602,  0.53792912,\n",
      "         0.11740328, -1.14606054,  2.30663965, -1.65128195, -1.71548593],\n",
      "       [ 1.35344287, -2.50864096, -1.03918023,  0.47824306, -0.48014763,\n",
      "        -0.16384341, -2.04974209, -2.38863439,  1.68323695, -1.33199457],\n",
      "       [ 1.61817278, -1.35271157, -2.75356501, -3.08493494,  2.32375017,\n",
      "         1.77353376,  2.73655823, -0.96704475, -1.69558765, -1.68826951],\n",
      "       [-0.76961086,  1.71979174, -1.73134789,  2.43960173, -1.94762033,\n",
      "        -2.58680382, -0.806014  ,  3.31125115, -0.27505743,  2.2661711 ],\n",
      "       [-0.54620016, -0.12466005,  2.06001494,  1.53665249,  1.93292328,\n",
      "         0.37692838, -0.58777595, -2.64512759,  0.54994383, -1.71565852],\n",
      "       [-0.47160862,  1.87272979,  0.63505768, -0.8625665 , -0.72582052,\n",
      "         0.45403238, -1.40107523,  1.91441341,  1.5490645 , -1.49011617]])]\n",
      "[array([[-1.29380152],\n",
      "       [ 0.22018275],\n",
      "       [-2.66785089],\n",
      "       [ 0.10774429],\n",
      "       [-0.36016759],\n",
      "       [-0.5405757 ],\n",
      "       [-0.6696817 ],\n",
      "       [ 1.74844535],\n",
      "       [-1.21524078],\n",
      "       [-0.48046994]]), array([[-0.13442632],\n",
      "       [-2.52567183],\n",
      "       [-0.0528253 ],\n",
      "       [ 0.3567898 ],\n",
      "       [ 2.14359399],\n",
      "       [ 3.28797232],\n",
      "       [ 1.50873038],\n",
      "       [-0.17089862],\n",
      "       [-0.75230295],\n",
      "       [-0.47682915]])]\n"
     ]
    }
   ],
   "source": [
    "w1,b1=update_weights_single_layer(X,Y,weights1,biases1,1)\n",
    "print(w1)\n",
    "print(b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "quqTP94Mkvyl"
   },
   "source": [
    "starting Neural Network with 2 hidden layers...\n",
    "\n",
    "**784>10>10>10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JTiyMFl8kvyn"
   },
   "outputs": [],
   "source": [
    "def update_weights_double_layer( X,Y,weights,biases, eta): \n",
    "        mini_batch_size=10 #mini_batch_size for stochastic GD\n",
    "        training_data = list(zip(X,Y))\n",
    "        n = len(training_data)\n",
    "        random.shuffle(training_data)\n",
    "        mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "        for mini_batch in mini_batches:\n",
    "            grad_b = [np.zeros(b.shape) for b in biases]\n",
    "            grad_w = [np.zeros(w.shape) for w in weights]\n",
    "            for x, y in mini_batch:\n",
    "                delta_grad_b, delta_grad_w = backprop(x, y,weights,biases)\n",
    "                grad_b = [nb+dnb for nb, dnb in zip(grad_b, delta_grad_b)]\n",
    "                grad_w = [nw+dnw for nw, dnw in zip(grad_w, delta_grad_w)]\n",
    "            weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(weights, grad_w)]\n",
    "            biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(biases, grad_b)]\n",
    "            \n",
    "        return weights,biases\n",
    "\n",
    "def backprop( x, y,weights,biases):\n",
    "    grad_b = [np.zeros(b.shape) for b in biases]  \n",
    "    grad_w = [np.zeros(w.shape) for w in weights]\n",
    "    # feedforward\n",
    "    activation = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    \n",
    "    for b, w in zip(biases[:-1], weights[:-1]):\n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "    for b, w in zip(biases[-1:], weights[-1:]):\n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = softmax(z)\n",
    "        activations.append(activation)\n",
    "    delta = delta1(zs[-1], activations[-1], y)\n",
    "    grad_b[-1] = delta\n",
    "    grad_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    for l in range(2, len(biases)+1):\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_prime(z)\n",
    "        delta = np.dot(weights[-l+1].transpose(), delta) *sp\n",
    "        grad_b[-l] = delta\n",
    "        grad_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    return (grad_b, grad_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3VLrSja6kvys"
   },
   "outputs": [],
   "source": [
    "sizes2=np.array([784,10,10,10])\n",
    "biases2 = [np.random.randn(y, 1) for y in sizes2[1:]]\n",
    "weights2 = [np.random.randn(y, x) for x, y in zip(sizes2[:-1], sizes2[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "g--AOQQkkvyy",
    "outputId": "6af902d4-3580-4cab-ac02-6c0f169cfd7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-1.01925631,  0.16662307, -1.25545279, ...,  0.06015003,\n",
      "         1.14296297,  0.63086857],\n",
      "       [-1.11716206,  0.47671476, -0.53007367, ..., -1.04651319,\n",
      "         0.60643914,  1.63261621],\n",
      "       [-1.72321606, -0.14129194,  0.56378852, ...,  1.19883135,\n",
      "         0.61797649,  0.20180285],\n",
      "       ...,\n",
      "       [-1.09939491,  0.6834544 ,  0.14031456, ...,  0.59316229,\n",
      "        -1.08269484, -0.7953746 ],\n",
      "       [-0.41263092, -1.63612621,  0.88505431, ...,  0.60924994,\n",
      "        -0.38211105,  1.08987879],\n",
      "       [ 0.43019252, -0.48654202, -0.07345832, ...,  0.19701464,\n",
      "        -0.04862068, -0.14561675]]), array([[ 0.03868441,  1.33759203, -1.5504352 , -0.90080828, -1.33621513,\n",
      "        -1.03490559, -0.05045638,  0.0552776 , -2.36018259, -1.40645622],\n",
      "       [ 2.11455707,  3.20961277,  0.27198415, -1.42177998,  0.37740354,\n",
      "         0.70193021, -1.49024721,  1.75290728, -0.55593166, -4.37744969],\n",
      "       [-0.96739501, -1.20845311,  1.34754281, -2.45032335,  0.37433583,\n",
      "        -2.96411759, -0.06673619,  0.35072119, -1.08280835,  3.52292783],\n",
      "       [-0.02719083, -0.35271018, -0.72009219,  1.6486225 ,  4.17604754,\n",
      "        -0.13715134,  1.24817186,  0.74532241,  0.67905571, -0.32381491],\n",
      "       [-1.07380551, -0.84096245,  0.9895491 , -3.47529342, -0.98101494,\n",
      "         0.58004271, -3.15282497, -2.1530693 ,  1.33495302, -0.43690976],\n",
      "       [ 1.70703034, -3.13554128,  0.11484807,  0.43489264, -1.20270692,\n",
      "        -0.40251024,  0.94492357,  1.12582979,  0.76771277, -0.21220384],\n",
      "       [ 0.28161109, -0.16971851,  1.70793094,  1.04473208,  1.09021897,\n",
      "        -3.77216295, -2.53303823, -0.94513746,  1.46605211, -1.39472136],\n",
      "       [ 0.51792094,  3.30067074, -0.03808691,  0.52355501,  0.04929582,\n",
      "        -0.02879824, -0.04737046, -2.47717652, -1.4876925 ,  1.84799304],\n",
      "       [ 1.03319847, -0.75594167, -2.72058315,  1.28376931, -1.29505775,\n",
      "        -1.35695068, -0.44815502, -2.41359216,  1.9402767 , -0.4618194 ],\n",
      "       [-0.1154341 , -1.57180544,  1.33940802, -0.07868315, -2.2012933 ,\n",
      "        -2.99218686,  1.71965114,  1.75639277, -0.2782444 , -0.89600549]]), array([[ 0.55431258, -0.02398094,  2.66928842,  0.23224478,  1.89281285,\n",
      "        -4.01675265,  2.00690484,  2.08559689, -2.31865879, -0.61327561],\n",
      "       [ 0.6536689 , -1.67484015, -4.23042686, -0.98145573, -3.23788074,\n",
      "         2.20121151,  2.39890261, -1.92771916,  2.34131769,  4.40134728],\n",
      "       [-1.14379627,  3.83817845, -0.58046616, -2.93365776,  2.30983095,\n",
      "         2.13870372, -0.42668269, -1.19469709, -0.82151246,  0.75210148],\n",
      "       [ 0.25092235, -1.35070972, -0.06700365, -0.75787379,  2.60560974,\n",
      "         0.88161259,  2.34704812, -2.12388081,  3.21403909, -0.62066919],\n",
      "       [ 0.52346575, -1.55435978, -1.8285719 ,  2.81388391, -3.88955277,\n",
      "         0.14794592, -3.49522619,  1.51112359,  0.38062239, -1.33334034],\n",
      "       [-1.05581743,  1.41364   , -0.90679813,  1.37921438, -0.49176442,\n",
      "        -1.61798554,  2.50458801,  2.44479276,  1.81886775, -0.81803855],\n",
      "       [ 0.84561529,  4.71153423, -2.47498156,  2.22095631, -0.67960596,\n",
      "        -1.32046898, -2.34008573, -0.43246249, -1.40954224, -1.2507673 ],\n",
      "       [ 0.23581505, -2.09572795,  4.40477325, -0.51207751, -1.98945958,\n",
      "         1.64083375, -1.05534328, -0.70955338, -0.77162065,  2.75412612],\n",
      "       [ 1.10403085,  1.17916115, -1.23057601, -2.6794624 ,  0.93141262,\n",
      "        -1.0352083 ,  0.70840188,  1.26871577,  2.7983429 ,  0.8459737 ],\n",
      "       [-1.74136359, -2.99859814,  0.52789697, -1.05509863, -0.10129441,\n",
      "         1.50480697, -4.15350117,  0.91377234,  2.0510642 ,  0.62817488]])]\n",
      "[array([[-0.798549  ],\n",
      "       [-0.42495921],\n",
      "       [ 0.33611136],\n",
      "       [ 1.003165  ],\n",
      "       [ 3.30297335],\n",
      "       [-0.94948263],\n",
      "       [ 0.16811722],\n",
      "       [ 0.16391628],\n",
      "       [ 1.1717006 ],\n",
      "       [-0.15489791]]), array([[ 0.63770013],\n",
      "       [-1.26614161],\n",
      "       [-1.00218402],\n",
      "       [-1.56301641],\n",
      "       [ 1.49700833],\n",
      "       [ 0.77363884],\n",
      "       [ 0.86711208],\n",
      "       [-0.76812034],\n",
      "       [ 1.54793364],\n",
      "       [-1.05212135]]), array([[-1.90679496],\n",
      "       [-0.55381327],\n",
      "       [ 0.64725225],\n",
      "       [-1.62447322],\n",
      "       [ 1.55614875],\n",
      "       [-1.97670668],\n",
      "       [-0.93590809],\n",
      "       [ 0.29178403],\n",
      "       [ 0.57491241],\n",
      "       [ 2.69439704]])]\n"
     ]
    }
   ],
   "source": [
    "w2,b2=update_weights_double_layer(X,Y,weights2,biases2,eta= 1)\n",
    "print(w2)\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0QtHTlBkvy4"
   },
   "source": [
    "Neural Network with 2 hidden layers with choices for choosing **activation function...**\n",
    "\n",
    "784>10>10>10\n",
    "\n",
    "Activation functions introduced:\n",
    "\n",
    "1) tanh\n",
    "\n",
    "2) relu\n",
    "\n",
    "3) sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7XEUJMakvy5"
   },
   "outputs": [],
   "source": [
    "def update_weights_double_layer_act( X,Y,weights,biases, eta,activation): \n",
    "    mini_batch_size=10\n",
    "    training_data = list(zip(X,Y))\n",
    "    n = len(training_data)\n",
    "    random.shuffle(training_data)\n",
    "    mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "    for mini_batch in mini_batches:\n",
    "        grad_b = [np.zeros(b.shape) for b in biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_grad_b, delta_grad_w = backprop(x, y,weights,biases,activation)\n",
    "            grad_b = [nb+dnb for nb, dnb in zip(grad_b, delta_grad_b)]\n",
    "            grad_w = [nw+dnw for nw, dnw in zip(grad_w, delta_grad_w)]\n",
    "        weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(weights, grad_w)]\n",
    "        biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(biases, grad_b)]\n",
    "\n",
    "    return weights,biases\n",
    "\n",
    "def backprop( x, y,weights,biases,act):\n",
    "\n",
    "    grad_b = [np.zeros(b.shape) for b in biases]\n",
    "    grad_w = [np.zeros(w.shape) for w in weights]\n",
    "    # feedforward\n",
    "    activation = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    if act=='tanh':\n",
    "        for b, w in zip(biases[:-1], weights[:-1]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = tanh(z)\n",
    "            activations.append(activation)\n",
    "        for b, w in zip(biases[-1:], weights[-1:]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = softmax(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = delta1(zs[-1], activations[-1], y)\n",
    "        grad_b[-1] = delta\n",
    "        grad_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, len(biases)+1):\n",
    "            z = zs[-l]\n",
    "            tp = tanh_grad(z)\n",
    "            delta = np.dot(weights[-l+1].transpose(), delta)*tp \n",
    "            grad_b[-l] = delta\n",
    "            grad_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    if act=='relu':\n",
    "        for b, w in zip(biases[:-1], weights[:-1]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = relu(z)\n",
    "            activations.append(activation)\n",
    "        for b, w in zip(biases[-1:], weights[-1:]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = softmax(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = delta1(zs[-1], activations[-1], y)\n",
    "        grad_b[-1] = delta\n",
    "        grad_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, len(biases)+1):\n",
    "            z = zs[-l]\n",
    "            rl = relu(z,deriv=True)\n",
    "            delta = np.dot(weights[-l+1].transpose(), delta)*rl\n",
    "            grad_b[-l] = delta\n",
    "            grad_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    if act=='sigmoid':\n",
    "        \n",
    "    \n",
    "        for b, w in zip(biases[:-1], weights[:-1]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        for b, w in zip(biases[-1:], weights[-1:]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = softmax(z)\n",
    "            activations.append(activation)\n",
    "        delta = delta1(zs[-1], activations[-1], y)\n",
    "        grad_b[-1] = delta\n",
    "        grad_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, len(biases)+1):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(weights[-l+1].transpose(), delta) *sp\n",
    "            grad_b[-l] = delta\n",
    "            grad_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        \n",
    "    return (grad_b, grad_w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2jdEgvCmkvzA"
   },
   "outputs": [],
   "source": [
    "sizes2=np.array([784,10,10,10])\n",
    "biases2 = [np.random.randn(y, 1) for y in sizes2[1:]]\n",
    "weights2 = [np.random.randn(y, x) for x, y in zip(sizes2[:-1], sizes2[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IdoxwE06kvzF",
    "outputId": "b9a63885-c5f8-452a-97db-c163d293d922"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[-0.24174226,  0.0165086 ,  0.63058535, ...,  0.94569143,\n",
       "           1.22747527, -0.18461562],\n",
       "         [-0.16716123, -1.57744561, -0.62000267, ...,  0.08415125,\n",
       "           0.8489621 , -1.72600231],\n",
       "         [ 0.22788913,  0.91846034, -0.39433575, ...,  1.34170934,\n",
       "          -1.19661036,  0.39180757],\n",
       "         ...,\n",
       "         [ 0.84684417,  0.49222717, -0.88528257, ...,  1.14381777,\n",
       "          -0.80802832, -0.67286737],\n",
       "         [-0.60176144, -0.40534512, -0.07173232, ...,  2.57037263,\n",
       "           0.37595739,  2.13928011],\n",
       "         [-0.22286425,  1.24262112,  1.05213377, ..., -0.26653912,\n",
       "          -1.49207114, -0.76708851]]),\n",
       "  array([[ 36.99995584, -18.63396918,   0.09496369,  19.45472073,\n",
       "          -15.57458329,   4.90061867, -18.17336142, -17.21602552,\n",
       "            6.75949479,   2.28743959],\n",
       "         [ -0.59432519,  17.41256297,  -0.45232943, -16.7699783 ,\n",
       "            6.33573495, -22.53907753,   3.76105047,  14.38008856,\n",
       "          -21.27112956,   0.96812334],\n",
       "         [ 14.23454268,   5.01013815,   0.943174  ,  -5.77138415,\n",
       "          -13.35097683, -19.30959797,  15.88186244,   0.64401443,\n",
       "          -41.5629585 ,  -0.48202344],\n",
       "         [-12.22214592,  -4.40990106,   0.6969663 ,  -6.24910108,\n",
       "           -1.09543124, -11.17715706,  -7.83559869,  45.8454031 ,\n",
       "           -5.14863055,  -1.58868259],\n",
       "         [ -5.84075466, -13.97294255,   1.82828579,   4.85086047,\n",
       "           11.49122097, -18.44580925,  25.67815674,  12.0947893 ,\n",
       "           18.44764977,  -1.81854579],\n",
       "         [ -0.33077751,   6.87270002,   0.41204789, -16.03741326,\n",
       "           37.43899377,  36.83029805,  -3.1418073 , -15.01911397,\n",
       "           -3.07075994,  -0.58035219],\n",
       "         [  0.31836329,  35.23157607,   0.12964191,  16.5525389 ,\n",
       "          -11.09307594,  25.61536091, -29.60961507,  -2.28983183,\n",
       "           18.25409783,   1.47346799],\n",
       "         [ 19.27350925,  14.25568728,  -1.33391667, -37.9674425 ,\n",
       "          -11.07790749, -11.01358713,  12.23590355,  16.73205042,\n",
       "           -4.59123877,   0.62303568],\n",
       "         [-13.58781729,  -5.12004889,   0.81835542,  -3.90976932,\n",
       "           11.57444699,  -8.76637484, -25.51883098,   5.97733946,\n",
       "           27.34224935,  -0.85796473],\n",
       "         [-13.71094575,  17.10910173,   2.44597271,  35.9667104 ,\n",
       "          -20.64860928, -12.93500166,  -0.77530778,  11.64111681,\n",
       "           -7.71224214,  -0.11704373]]),\n",
       "  array([[ 1.62986770e+00, -1.43044171e+00, -1.38665872e+00,\n",
       "          -1.15578639e+00, -5.25846726e-01,  1.19118769e+00,\n",
       "           2.56797761e+00,  1.27344027e+00,  3.76491616e-01,\n",
       "          -9.64330444e-01],\n",
       "         [-3.21176627e+00,  9.26172941e-03, -1.06347972e+00,\n",
       "           1.53372585e-01, -3.20702322e+00, -1.43939642e+00,\n",
       "          -3.52816117e+00, -1.04574983e+00,  4.28600612e-01,\n",
       "          -1.50683278e+00],\n",
       "         [ 1.81641788e-01, -2.28582704e+00, -5.22471346e-01,\n",
       "          -1.59509890e+00,  1.66997441e+00, -1.52363851e+00,\n",
       "           1.23772200e+00,  3.80565202e-01,  1.62037042e+00,\n",
       "          -1.13247717e+00],\n",
       "         [-1.08615188e+00,  3.99956109e-01, -1.45356264e+00,\n",
       "           2.51774678e+00,  5.86602650e-01,  8.63733295e-02,\n",
       "          -2.84180292e-01,  1.85165839e+00,  1.47929155e+00,\n",
       "          -2.15131109e-01],\n",
       "         [-7.80048579e-01, -1.55680033e-01,  1.31350373e+00,\n",
       "           4.49503019e-01,  2.37041488e+00, -8.59833828e-01,\n",
       "          -9.02813065e-01, -1.19709035e-01, -8.41877002e-01,\n",
       "           7.09599734e-01],\n",
       "         [-1.03696517e+00,  1.12973952e+00,  9.91044660e-01,\n",
       "           2.13528470e-01, -1.59449386e+00,  5.59730271e-01,\n",
       "           8.40950395e-01,  2.32527186e+00, -3.99182810e-01,\n",
       "           4.69925371e-01],\n",
       "         [ 7.13400731e-01,  5.96249625e-01, -7.83524153e-01,\n",
       "          -1.73481640e+00, -3.13601515e-01, -1.76711167e+00,\n",
       "           1.51820292e+00,  2.90084557e-01,  2.66954158e-03,\n",
       "           1.73601480e+00],\n",
       "         [-1.68525142e+00, -7.27725395e-01, -2.22105513e-01,\n",
       "           1.28467775e-01,  1.96028115e+00,  2.84392508e+00,\n",
       "          -9.68402750e-01, -8.12628830e-01,  1.00453246e+00,\n",
       "          -1.18255589e+00],\n",
       "         [ 1.30325025e+00, -6.92482053e-01,  1.26460798e+00,\n",
       "           5.03622686e-01, -9.74081534e-01, -1.06325122e+00,\n",
       "          -1.25446157e+00,  2.03619269e+00, -4.47447615e-01,\n",
       "          -6.15047178e-01],\n",
       "         [-9.70243619e-01,  1.32080545e+00,  1.15518037e+00,\n",
       "           6.95877848e-01,  2.74751267e+00,  8.39052662e-01,\n",
       "          -4.87257003e-01,  1.54796921e+00, -1.50792839e+00,\n",
       "          -2.12942654e+00]])],\n",
       " [array([[-85.35293326],\n",
       "         [316.55371354],\n",
       "         [ -0.91029825],\n",
       "         [287.65331249],\n",
       "         [196.22803762],\n",
       "         [191.30048254],\n",
       "         [299.14993991],\n",
       "         [-72.03126598],\n",
       "         [220.99040678],\n",
       "         [ -1.71127281]]), array([[ 0.34608271],\n",
       "         [ 3.33949457],\n",
       "         [ 5.67233637],\n",
       "         [ 9.76791284],\n",
       "         [-0.03761649],\n",
       "         [ 4.31081022],\n",
       "         [ 0.19471911],\n",
       "         [-2.9004927 ],\n",
       "         [ 0.06492626],\n",
       "         [ 6.72412918]]), array([[-2.86463442],\n",
       "         [ 4.4504973 ],\n",
       "         [ 0.4169001 ],\n",
       "         [-1.20939542],\n",
       "         [-1.97650065],\n",
       "         [-0.25809903],\n",
       "         [-2.48784003],\n",
       "         [-1.12535118],\n",
       "         [ 0.71542253],\n",
       "         [-1.48476955]])])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_weights_double_layer_act(X,Y,weights2,biases2, 1,activation='relu') #(activation={'tanh','relu','sigmoid'})  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kAxWVi5XkvzM"
   },
   "source": [
    "Implementing **momentum** with gradient descent and adding **epochs** for multiple iterations... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qU7Ji5cCkvzO"
   },
   "outputs": [],
   "source": [
    "def update_weights_double_layer_mom(X,Y,weights,biases, eta,momentum,epochs,activation): #mini-batches for stochastic GD\n",
    "    mini_batch_size=10\n",
    "    beta=momentum\n",
    "    training_data = list(zip(X,Y))\n",
    "    n = len(training_data)\n",
    "    for j in range (epochs):\n",
    "        random.shuffle(training_data)\n",
    "        mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "        for mini_batch in mini_batches:\n",
    "            grad_b = [np.zeros(b.shape) for b in biases]\n",
    "            grad_w = [np.zeros(w.shape) for w in weights]\n",
    "            for x, y in mini_batch:\n",
    "                delta_grad_b, delta_grad_w = backprop(x, y,weights,biases,activation)\n",
    "                grad_b = [nb+dnb for nb, dnb in zip(grad_b, delta_grad_b)]\n",
    "                grad_w = [nw+dnw for nw, dnw in zip(grad_w, delta_grad_w)]\n",
    "            #momentum implementation\n",
    "            #VdW = β x VdW + (1 – β) x dW\n",
    "            vdw=0\n",
    "            vdw = [vdw*beta+(1-beta)*nab_w for nab_w in grad_w]\n",
    "            #Vdb = β x Vdb + (1 – β) x db\n",
    "            vdb=0\n",
    "            vdb = [vdb*beta+(1-beta)*nab_b for nab_b in grad_b]\n",
    "            weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(weights, vdw)]\n",
    "            biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(biases, vdb)]\n",
    "        print(\"epoch:\",j+1,\"done\")\n",
    "    return weights,biases\n",
    "\n",
    "\n",
    "def backprop( x, y,weights,biases,act):\n",
    "\n",
    "    grad_b = [np.zeros(b.shape) for b in biases]\n",
    "    grad_w = [np.zeros(w.shape) for w in weights]\n",
    "    # feedforward\n",
    "    activation = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    if act=='tanh':\n",
    "        for b, w in zip(biases[:-1], weights[:-1]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = tanh(z)\n",
    "            activations.append(activation)\n",
    "        for b, w in zip(biases[-1:], weights[-1:]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = softmax(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = delta1(zs[-1], activations[-1], y)\n",
    "        grad_b[-1] = delta\n",
    "        grad_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, len(biases)+1):\n",
    "            z = zs[-l]\n",
    "            tp = tanh_grad(z)\n",
    "            delta = np.dot(weights[-l+1].transpose(), delta)*tp \n",
    "            grad_b[-l] = delta\n",
    "            grad_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    if act=='relu':\n",
    "        for b, w in zip(biases[:-1], weights[:-1]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = relu(z)\n",
    "            activations.append(activation)\n",
    "        for b, w in zip(biases[-1:], weights[-1:]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = softmax(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = delta1(zs[-1], activations[-1], y)\n",
    "        grad_b[-1] = delta\n",
    "        grad_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, len(biases)+1):\n",
    "            z = zs[-l]\n",
    "            rl = relu(z,deriv=True)\n",
    "            delta = np.dot(weights[-l+1].transpose(), delta)*rl\n",
    "            grad_b[-l] = delta\n",
    "            grad_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    if act=='sigmoid':\n",
    "        \n",
    "    \n",
    "        for b, w in zip(biases[:-1], weights[:-1]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        for b, w in zip(biases[-1:], weights[-1:]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = softmax(z)\n",
    "            activations.append(activation)\n",
    "        delta = delta1(zs[-1], activations[-1], y)\n",
    "        grad_b[-1] = delta\n",
    "        grad_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, len(biases)+1):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(weights[-l+1].transpose(), delta) *sp\n",
    "            grad_b[-l] = delta\n",
    "            grad_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        \n",
    "    return (grad_b, grad_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHgzfM3okvzS"
   },
   "outputs": [],
   "source": [
    "sizes2=np.array([784,10,10,10])\n",
    "biases2 = [np.random.randn(y, 1) for y in sizes2[1:]]\n",
    "weights2 = [np.random.randn(y, x) for x, y in zip(sizes2[:-1], sizes2[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "22sk8DFYkvzb",
    "outputId": "dcbd1eb0-a6dd-4a08-a982-7bff8430b127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 done\n",
      "epoch: 2 done\n",
      "epoch: 3 done\n",
      "epoch: 4 done\n",
      "epoch: 5 done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([[-0.24174226,  0.0165086 ,  0.63058535, ...,  0.94569143,\n",
       "           1.22747527, -0.18461562],\n",
       "         [-0.16716123, -1.57744561, -0.62000267, ...,  0.08415125,\n",
       "           0.8489621 , -1.72600231],\n",
       "         [ 0.22788913,  0.91846034, -0.39433575, ...,  1.34170934,\n",
       "          -1.19661036,  0.39180757],\n",
       "         ...,\n",
       "         [ 0.84684417,  0.49222717, -0.88528257, ...,  1.14381777,\n",
       "          -0.80802832, -0.67286737],\n",
       "         [-0.60176144, -0.40534512, -0.07173232, ...,  2.57037263,\n",
       "           0.37595739,  2.13928011],\n",
       "         [-0.22286425,  1.24262112,  1.05213377, ..., -0.26653912,\n",
       "          -1.49207114, -0.76708851]]),\n",
       "  array([[ 1.09921856e+01, -8.09155609e+00, -2.19578888e+00,\n",
       "           2.07486989e+01, -2.18110307e+01,  1.16706481e+00,\n",
       "          -1.71062710e+00, -4.78602999e+00,  2.94889317e+01,\n",
       "           1.94709364e+01],\n",
       "         [ 5.79091719e+00,  8.15864520e+00,  8.30668292e+00,\n",
       "           5.74722524e+00,  8.03600541e+00, -3.73458715e+00,\n",
       "          -2.03795810e+00,  1.40505616e+01, -2.62716783e+01,\n",
       "          -2.08241997e+00],\n",
       "         [ 9.53444742e+00, -1.82058852e+01,  1.24119898e+01,\n",
       "           3.49626007e+01, -8.33007680e+00, -1.13844961e+00,\n",
       "           1.34699373e+01,  2.65644729e+01, -2.75084163e+00,\n",
       "          -5.76756291e+00],\n",
       "         [-6.11896974e+00,  1.23618908e+01, -1.68053234e+00,\n",
       "          -1.00689173e+01, -2.52386083e+00,  3.29367093e+01,\n",
       "           2.43061940e+00,  3.76903206e+01,  2.60715977e+00,\n",
       "          -2.62308534e+00],\n",
       "         [-6.43383119e+00,  2.77614434e+00,  3.72035660e+01,\n",
       "           3.16286929e+00,  1.52656515e+01,  1.54684421e+01,\n",
       "           3.87505255e+01, -1.06173326e+01,  1.86703236e+01,\n",
       "          -2.08484746e+01],\n",
       "         [ 3.64394180e+01, -3.19510103e+00, -7.10095994e+00,\n",
       "          -5.36024269e+00,  2.57229143e+01,  3.62991077e+01,\n",
       "           1.69344236e+01, -5.53906568e-01, -1.36360822e+00,\n",
       "          -4.21446402e+00],\n",
       "         [ 3.52732689e+01,  2.99564897e+01,  9.70471384e+00,\n",
       "           7.48315689e+00,  1.92713378e+01, -5.65115138e+00,\n",
       "          -1.12591001e+01, -7.05905057e+00,  1.34897794e+01,\n",
       "           2.87772105e+01],\n",
       "         [ 8.47444722e+00,  1.93248139e+01, -5.43192055e+00,\n",
       "          -5.87595830e+00, -9.48767070e+00,  1.60293287e+01,\n",
       "          -8.88208871e+00, -3.88403996e-01, -1.00135170e+01,\n",
       "           1.52204778e+01],\n",
       "         [-3.31548270e+01,  1.06788043e+01,  8.47035169e+00,\n",
       "          -9.43767967e-01, -7.86166111e+00, -4.82000547e+00,\n",
       "          -7.35054987e+00,  1.96533103e+01,  1.91500835e+01,\n",
       "           7.05367886e+00],\n",
       "         [-1.13705018e-02,  5.66054077e+00,  2.97750893e+01,\n",
       "           2.31481829e+01, -1.20381248e+01,  1.02410480e+01,\n",
       "           8.24365877e+00,  2.04896717e-02, -2.02177192e+01,\n",
       "           1.61794046e+01]]),\n",
       "  array([[ 0.53696931, -1.54346084, -1.85481881, -2.29647515, -0.80275195,\n",
       "           1.42901985,  2.09408094,  1.4955047 , -0.22995238, -1.66405054],\n",
       "         [-1.76200178,  0.19697843,  0.97191803,  1.4284773 , -1.97135578,\n",
       "          -1.0290379 , -2.94500341,  0.05488683,  0.86430174, -0.47904387],\n",
       "         [ 0.22408487, -2.3207243 , -0.6161769 ,  0.29085646,  0.79902691,\n",
       "          -1.6239525 ,  1.39498253,  0.28764222,  2.05976217, -3.09539683],\n",
       "         [-1.40785339, -0.25630024, -2.39746211,  2.21572203, -0.55598767,\n",
       "           0.06461996, -0.37974486,  2.58901872,  1.67156314,  0.33221748],\n",
       "         [-0.77875284, -0.08898398,  1.46123034, -0.4979425 ,  3.32314991,\n",
       "          -0.62357576, -1.62092826, -0.24039885, -0.84996565,  1.31461674],\n",
       "         [-1.91257257,  2.06082174,  0.18179009, -0.30459354, -0.91266093,\n",
       "          -0.18586566,  1.65552882,  1.67040669, -0.52324999,  0.4632492 ],\n",
       "         [ 0.45596361,  0.60927147,  0.11955076, -1.80548966,  0.93856488,\n",
       "          -3.41864776,  0.38498644,  0.78279209,  0.27586633,  2.52471615],\n",
       "         [-1.59811411, -0.72323026, -0.85980999,  0.12184805,  1.50861049,\n",
       "           2.93172908, -0.49984026, -1.18750458,  1.24263999, -3.03551644],\n",
       "         [ 1.58784736, -0.90655815,  1.59532055,  0.5294145 , -1.51803406,\n",
       "           0.43427876, -0.85889474,  1.06668652, -0.47614937, -0.03169951],\n",
       "         [-0.28783692,  1.13604235,  0.69099268,  0.49459996,  1.91117711,\n",
       "           0.88846932, -0.48559012,  1.20806013, -2.31929562, -1.15935359]])],\n",
       " [array([[257.14170556],\n",
       "         [ 82.45127115],\n",
       "         [187.59207612],\n",
       "         [ 63.68931795],\n",
       "         [192.35145426],\n",
       "         [184.05104312],\n",
       "         [139.12336227],\n",
       "         [ -2.66380687],\n",
       "         [-49.23112166],\n",
       "         [-16.04877216]]), array([[-3.9549075 ],\n",
       "         [ 6.0139264 ],\n",
       "         [ 3.89432596],\n",
       "         [ 4.99232395],\n",
       "         [ 6.1632656 ],\n",
       "         [ 5.01945945],\n",
       "         [ 5.2308885 ],\n",
       "         [-2.67191632],\n",
       "         [-1.32055542],\n",
       "         [ 4.80018153]]), array([[-1.8945426 ],\n",
       "         [ 2.89815503],\n",
       "         [ 1.31646885],\n",
       "         [-0.79312007],\n",
       "         [-2.20229483],\n",
       "         [ 0.19541229],\n",
       "         [-0.84798116],\n",
       "         [-1.07041279],\n",
       "         [-1.62505933],\n",
       "         [-1.80039574]])])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_weights_double_layer_mom(X,Y,weights2,biases2,eta=1,momentum=0.9,epochs=5,activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qpxypksukvzh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment2_MS16016.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
